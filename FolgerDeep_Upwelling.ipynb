{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85a3b2e",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to explore some data from **Folger Deep**.  The steps in this notebook are:\n",
    "- Look at the structure of the scalar data file (CM and ABC data)\n",
    "\n",
    "- Look at Upwelling At Folger Deep, use 24 hour average ABC and CM\n",
    "  - Load the ABC and CM data\n",
    "  - Load some prepared upwelling data from a co-located ADCP\n",
    "  - Plot the two time-series for Two Years (2018-Jan-01 to 2020-Jan-01) to help identify some interesting data\n",
    "\n",
    "- Look at the structure of the MVBS complex data file\n",
    "- Load and Plot the MVBS data for two months (June 5th to July 15th, 2019)\n",
    "- Load the CM data for the functional groups, and plot it over top of the data (June 25th to July 5th, 2019)\n",
    "- Download the Original Sv data from Folger Deep using the ONC Client Library, and see how it compares to the MVBS data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the relative paths of the contents of this repo\n",
    "\n",
    "from pathlib import Path\n",
    "MVBS_folder = Path.cwd() / 'MVBS_examples'\n",
    "scalar_folder = Path.cwd() / 'CM_ABC_averaged_examples'\n",
    "ADCP_folder = Path.cwd() / 'extra_data'\n",
    "\n",
    "scalar_filename = scalar_folder / \"FolgerDeep_20140508_20251230_averaged.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b24f8",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "Look at the structure of the ABC and CM time-series file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def print_mat_structure(filename, root=\"data_avg\"):\n",
    "    \"\"\"\n",
    "    Print structure of a MATLAB v7.3 MAT file in a MATLAB-like format.\n",
    "    \"\"\"\n",
    "\n",
    "    def _print_item(name, obj, indent=0):\n",
    "        pad = \"  \" * indent\n",
    "\n",
    "        if isinstance(obj, h5py.Group):\n",
    "            print(f\"{pad}{name.split('/')[-1]}\")\n",
    "            for key in obj.keys():\n",
    "                _print_item(f\"{name}/{key}\", obj[key], indent + 1)\n",
    "\n",
    "        elif isinstance(obj, h5py.Dataset):\n",
    "            shape = obj.shape\n",
    "            dtype = obj.dtype\n",
    "            print(f\"{pad}{name.split('/')[-1]} : {dtype} {shape}\")\n",
    "\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        if root not in f:\n",
    "            raise ValueError(f\"Root '{root}' not found in file\")\n",
    "\n",
    "        _print_item(root, f[root], indent=0)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print_mat_structure(scalar_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649c0aa",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "Look at Upwelling At Folger Deep, use 24 hour average ABC and CM\n",
    "- Load the ABC and CM data\n",
    "- Load some prepared upwelling data from a co-located ADCP\n",
    "- Plot the two time-series to help identify some interesting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c883ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the CM, and ABC for Folger Deep\n",
    "# Use the \"pre-averaged\" data to make cleaner plots (use 24 hr average instead of 5 minute average)\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#To convert MATLAB datenum to python datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Look at data from Folger Deep\n",
    "filename = scalar_filename\n",
    "\n",
    "with h5py.File(filename,'r') as f:\n",
    "    #Import Daily averaged data\n",
    "    data = f['data_avg']['hr_24']\n",
    "\n",
    "    #Load time variable and convert to datetime\n",
    "    time_MATLAB = np.array(data['time']).squeeze()    \n",
    "    # The offset for the Unix epoch (1970-01-01) in MATLAB datenum format is 719529\n",
    "    unix_epoch_offset_matlab = 719529\n",
    "    time = pd.to_datetime(time_MATLAB - unix_epoch_offset_matlab, unit='D')\n",
    "\n",
    "    #Get the Time-series data for the 38 kHz channel\n",
    "    ch_38 = data['ch_38kHz']\n",
    "    #ch_38 = data['ch_38kHz_fish']\n",
    "    CM_38 = np.array(ch_38['CM'])\n",
    "    ABC_38 = np.array(ch_38['ABC'])\n",
    "\n",
    "    #Get the Time-series data for the 120 kHz channel\n",
    "    ch_120 = data['ch_120kHz']\n",
    "    #ch_120 = data['ch_120kHz_krill2']\n",
    "    CM_120 = np.array(ch_120['CM'])\n",
    "    ABC_120 = np.array(ch_120['ABC'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b558f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load vertical displacement from the co-located ADCP\n",
    "#\n",
    "# This is derived by taking the cumulative sum of the vertical speed at each given depth bin\n",
    "# This does not represent the total vertical flow over the entire water column\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from itertools import islice\n",
    "\n",
    "def load_csv_pandas(path):\n",
    "    # skip first 3 lines, use the 4th line as header\n",
    "    df = pd.read_csv(path, skiprows=3)\n",
    "    return df  # or: return df.to_dict(orient='records') for list-of-dicts\n",
    "\n",
    "#Specify the input file:\n",
    "csv_filename = 'ADCP300kHz_FolgerDeep_2014_2026_verticalDisplacement.csv'\n",
    "#csv_filename = 'ADCP300kHz_FolgerDeep_2014_2026_verticalFlowSpeed.csv' #Alternate data is flow in m/day\n",
    "df = load_csv_pandas(ADCP_folder / csv_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using vertical displacement, do this\n",
    "\n",
    "# Convert the date and time columns to a single datetime column\n",
    "verticalDisplacement_time = pd.to_datetime(df[['year','month','day','hour','minute','second']])\n",
    "verticalDisplacement_midColumn = df['52.9 m']\n",
    "verticalDisplacement_upperColumn = df['16.9 m']\n",
    "\n",
    "#To make the signal stand out in the plots more clearly, we can \n",
    "# detrend the vertical displacement data by removing the linear trend. \n",
    "# This will help to highlight any variations in the data that are not due \n",
    "# to a simple linear increase or decrease over time.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def trend_detrend(series):\n",
    "    mask = series.notna()\n",
    "    if isinstance(series.index, pd.DatetimeIndex):\n",
    "        t = series.index.view('int64')/1e9  # seconds since epoch\n",
    "    else:\n",
    "        t = np.arange(len(series))\n",
    "    coef = np.polyfit(t[mask], series[mask], 1)\n",
    "    trend_all = np.polyval(coef, t)\n",
    "    detrended = series - trend_all\n",
    "    detrended[~mask] = np.nan  # restore NaNs\n",
    "    return detrended\n",
    "\n",
    "verticalDisplacement_midColumn_detrend = trend_detrend(verticalDisplacement_midColumn)\n",
    "verticalDisplacement_upperColumn_detrend = trend_detrend(verticalDisplacement_upperColumn)\n",
    "\n",
    "\n",
    "#if csv_filename == 'ADCP300kHz_FolgerDeep_2014_2026_verticalFlowSpeed.csv':\n",
    "#    verticalDisplacement_time = pd.to_datetime(df[['year','month','day','hour','minute','second']])\n",
    "#    verticalFlow_midColumn = df['52.9 m']\n",
    "#    verticalFlow_upperColumn = df['16.9 m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa519019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Zoom to a date range\n",
    "start_zoom = datetime(2018, 1, 1)\n",
    "end_zoom = datetime(2020, 1, 1)\n",
    "#start_zoom = datetime(2024, 1, 1)\n",
    "#end_zoom = datetime(2026, 1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "axes[0].plot(time, ABC_38[1,:])  # Plot the second column of CM_38 against time\n",
    "axes[0].plot(time, ABC_120[1,:])  # Plot the second column of CM_120 against time\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('ABC m^2 / m^-2')\n",
    "axes[0].set_xlim(start_zoom, end_zoom)\n",
    "axes[0].set_ylim(0, 4*10**-4)\n",
    "axes[0].legend(['38 kHz', '120 kHz'])\n",
    "axes[0].grid(True)\n",
    "\n",
    "#Also, plot some red vertical lines to highlight where upwelling appears to result in a bloom\n",
    "line_dates = [datetime(2019, 6, 5), datetime(2019, 7, 15)]\n",
    "# line_dates = [datetime(2025, 9, 5), datetime(2025, 10, 15)]\n",
    "for line_date in line_dates:\n",
    "    axes[0].plot( [line_date, line_date], [0, 1],'r--')  \n",
    "    \n",
    "\n",
    "if csv_filename == 'ADCP300kHz_FolgerDeep_2014_2026_verticalFlowSpeed.csv':\n",
    "    axes[1].plot(verticalDisplacement_time, verticalFlow_midColumn)  \n",
    "    axes[1].set_ylabel('Detrended Vertical \\n Flow (m / day)')\n",
    "    #axes[1].set_ylim(-0.5e7, 1e7)   \n",
    "\n",
    "elif csv_filename == 'ADCP300kH_FolgerDeep_2014_2026_verticalDisplacement.csv':\n",
    "    axes[1].plot(verticalDisplacement_time, verticalDisplacement_midColumn_detrend)  \n",
    "    # axes[1].plot(verticalDisplacement_time, verticalDisplacement_upperColumn_detrend)  \n",
    "    #axes[2].plot(verticalDisplacement_time, verticalDisplacement_midColumn)  \n",
    "    axes[1].set_ylabel('Detrended Vertical \\n Displacement(m)')\n",
    "    axes[1].set_ylim(-1.1e7, -0.2e7)   # 2018-2019\n",
    "    #axes[1].set_ylim(-0.5e7, 1e7)  # 2024-2025\n",
    "\n",
    "axes[1].set_xlabel('Time')          \n",
    "axes[1].set_xlim(start_zoom, end_zoom)\n",
    "# axes[1].legend(['mid-column (52.9 m)', 'upper-column (16.9 m)'])\n",
    "axes[1].grid(True)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a301fed",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "Look at the structure of the MVBS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the filenames of the corresponding MVBS data\n",
    "MVBS_filename = []\n",
    "MVBS_filename.append(MVBS_folder / \"FolgerDeep_BIOSONICSDTXU08003_20190601T000000_20190701T000000_MVBS_300s_1m.mat\")\n",
    "MVBS_filename.append(MVBS_folder / \"FolgerDeep_BIOSONICSDTXU08003_20190701T000000_20190801T000000_MVBS_300s_1m.mat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef416c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def print_mat_structure(filename, root=\"data\"):\n",
    "    \"\"\"\n",
    "    Print structure of a MATLAB v7.3 MAT file in a MATLAB-like format.\n",
    "    \"\"\"\n",
    "\n",
    "    def _print_item(name, obj, indent=0):\n",
    "        pad = \"  \" * indent\n",
    "\n",
    "        if isinstance(obj, h5py.Group):\n",
    "            print(f\"{pad}{name.split('/')[-1]}\")\n",
    "            for key in obj.keys():\n",
    "                _print_item(f\"{name}/{key}\", obj[key], indent + 1)\n",
    "\n",
    "        elif isinstance(obj, h5py.Dataset):\n",
    "            shape = obj.shape\n",
    "            dtype = obj.dtype\n",
    "            print(f\"{pad}{name.split('/')[-1]} : {dtype} {shape}\")\n",
    "\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        if root not in f:\n",
    "            raise ValueError(f\"Root '{root}' not found in file\")\n",
    "\n",
    "        _print_item(root, f[root], indent=0)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print_mat_structure(MVBS_filename[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b14cc",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Load and Plot the MVBS data for 2019-Jun-01 to 2019-Aug-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#To convert MATLAB datenum to python datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Create keys and a dict for the MVBS\n",
    "channels = ['ch_38kHz','ch_123kHz','ch_210kHz','ch_38kHz_fish','ch_123kHz_krill','ch_123kHz_krill2'] \n",
    "MVBS = {k: [] for k in channels}\n",
    "MVBS_list = {k: [] for k in channels}\n",
    "\n",
    "#Initialize lists for loading the data from multiple files\n",
    "time_MATLAB_list = [] #Time variable\n",
    "\n",
    "\n",
    "for filename in MVBS_filename:\n",
    "    with h5py.File(filename,'r') as f:\n",
    "        #Import hourly averaged data\n",
    "        data = f['data']\n",
    "\n",
    "        #Load time variable and convert to datetime\n",
    "        time_MATLAB_list.append(np.array(data['time']).squeeze())    \n",
    "\n",
    "        #Get the Time-series data for the 38 kHz channel\n",
    "        MVBS_load = data['MVBS']\n",
    "        for ch in channels:\n",
    "            MVBS_list[ch].append(np.array(MVBS_load[ch]))\n",
    "\n",
    "        #Load the bin depth data. This will be consistent between files\n",
    "        depth_data = np.array(data['depth_bins'])\n",
    "\n",
    "\n",
    "#After loading from all files, concatenate the arrays\n",
    "for ch in channels:\n",
    "    MVBS[ch] = np.concatenate(MVBS_list[ch], axis=1)\n",
    "\n",
    "#Also, concatenate the time values, and convert to datetime\n",
    "# The offset for the Unix epoch (1970-01-01) in MATLAB datenum format is 719529\n",
    "unix_epoch_offset_matlab = 719529\n",
    "time_MATLAB = np.concatenate(time_MATLAB_list)\n",
    "time = pd.to_datetime(time_MATLAB - unix_epoch_offset_matlab, unit='D')\n",
    "\n",
    "#Delete the list variables to free up memory\n",
    "del MVBS_list, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Plot MVBS for the 3 channels\n",
    "# 2) Plot the CM on top of the MVBS data\n",
    "\n",
    "# Zoom to a date range\n",
    "start_zoom = datetime(2019, 6, 5)\n",
    "end_zoom = datetime(2019, 7, 15)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Set background color for all subplots\n",
    "for ax in axes:\n",
    "    ax.set_facecolor([0.0, 0.0, 0.2])  # Dark Blue\n",
    "\n",
    "#Plot each channel \n",
    "im_list = []\n",
    "for ii, ch in enumerate(channels[0:3]):\n",
    "    im = axes[ii].imshow(MVBS[ch],\n",
    "                   extent = [time.min(), time.max(), depth_data.max(), depth_data.min()],\n",
    "                   aspect = 'auto',\n",
    "                   origin = 'lower',\n",
    "                   vmin=-70,\n",
    "                   vmax=-45,\n",
    "                   cmap='turbo')\n",
    "    im_list.append(im)  # keep reference for colorbar\n",
    "    \n",
    "    axes[ii].set_title(ch)\n",
    "    axes[ii].set_xlabel('Time')\n",
    "    axes[ii].set_ylabel('Depth [m]')\n",
    "    axes[ii].set_ylim(90, 0)\n",
    "    #Zoom to the days when the bloom in productivity appears to happen from the ABC data\n",
    "    axes[ii].set_xlim(start_zoom, end_zoom)\n",
    "    \n",
    "\n",
    "# Add a single colorbar for all subplots\n",
    "fig.colorbar(im_list[0], ax=axes, orientation='vertical', fraction=0.05, pad=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf85881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Plot MVBS for the 3 channels\n",
    "# 2) Plot the CM on top of the MVBS data\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Set background color for all subplots\n",
    "for ax in axes:\n",
    "    ax.set_facecolor([0.0, 0.0, 0.2])  # Dark Blue\n",
    "\n",
    "#Plot each channel \n",
    "im_list = []\n",
    "for ii, ch in enumerate(channels[3:]):\n",
    "    im = axes[ii].imshow(MVBS[ch],\n",
    "                   extent = [time.min(), time.max(), depth_data.max(), depth_data.min()],\n",
    "                   aspect = 'auto',\n",
    "                   origin = 'lower',\n",
    "                   vmin=-70,\n",
    "                   vmax=-45,\n",
    "                   cmap='turbo')\n",
    "    im_list.append(im)  # keep reference for colorbar\n",
    "    \n",
    "    axes[ii].set_title(ch)\n",
    "    axes[ii].set_xlabel('Time')\n",
    "    axes[ii].set_ylabel('Depth [m]')\n",
    "    axes[ii].set_ylim(90, 0)\n",
    "\n",
    "    #Zoom to the days when the bloom in productivity appears to happen from the ABC data\n",
    "    axes[ii].set_xlim(start_zoom, end_zoom)\n",
    "\n",
    "# Add a single colorbar for all subplots\n",
    "fig.colorbar(im_list[0], ax=axes, orientation='vertical', fraction=0.05, pad=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817ffb4",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Zoom in closer and plot the CM on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aae2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the 1 hour averaged CM\n",
    "filename = scalar_filename\n",
    "\n",
    "with h5py.File(filename,'r') as f:\n",
    "    #Import Daily averaged data\n",
    "    data = f['data_avg']['hr_01']\n",
    "\n",
    "    #Load time variable and convert to datetime\n",
    "    time_MATLAB = np.array(data['time']).squeeze()    \n",
    "    # The offset for the Unix epoch (1970-01-01) in MATLAB datenum format is 719529\n",
    "    unix_epoch_offset_matlab = 719529\n",
    "    time_CM = pd.to_datetime(time_MATLAB - unix_epoch_offset_matlab, unit='D')\n",
    "\n",
    "    # Get the CM for the functional groups\n",
    "    CM_38_fish = np.array(data['ch_38kHz_fish']['CM'])\n",
    "    CM_123_krill = np.array(data['ch_120kHz_krill']['CM'])\n",
    "    CM_123_krill2 = np.array(data['ch_120kHz_krill2']['CM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the MVBS and 1 hour averaged CM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Plot MVBS for the 3 channels\n",
    "# 2) Plot the CM on top of the MVBS data\n",
    "\n",
    "# Zoom to a date range\n",
    "start_zoom = datetime(2019, 6, 25)\n",
    "end_zoom = datetime(2019, 7, 5)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Set background color for all subplots\n",
    "for ax in axes:\n",
    "    ax.set_facecolor([0.0, 0.0, 0.2])  # Dark Blue\n",
    "\n",
    "#Plot each channel \n",
    "im_list = []\n",
    "for ii, ch in enumerate(channels[3:6]):\n",
    "    im = axes[ii].imshow(MVBS[ch],\n",
    "                   extent = [time.min(), time.max(), depth_data.max(), depth_data.min()],\n",
    "                   aspect = 'auto',\n",
    "                   origin = 'lower',\n",
    "                   vmin=-70,\n",
    "                   vmax=-45,\n",
    "                   cmap='turbo')\n",
    "    im_list.append(im)  # keep reference for colorbar\n",
    "    \n",
    "    axes[ii].set_title(ch)\n",
    "    axes[ii].set_xlabel('Time')\n",
    "    axes[ii].set_ylabel('Depth [m]')\n",
    "    axes[ii].set_ylim(90, 0)\n",
    "    #Zoom to the days when the bloom in productivity appears to happen from the ABC data\n",
    "    axes[ii].set_xlim(start_zoom, end_zoom)\n",
    "    \n",
    "    #Plot the CM on top\n",
    "    if ii == 0:\n",
    "        axes[ii].plot(time_CM,CM_38_fish[0],'r--')\n",
    "    elif ii == 1:\n",
    "        axes[ii].plot(time_CM,CM_123_krill[0],'r--')\n",
    "    elif ii == 2:\n",
    "        axes[ii].plot(time_CM,CM_123_krill2[0],'r--')\n",
    "    \n",
    "\n",
    "\n",
    "# Add a single colorbar for all subplots\n",
    "fig.colorbar(im_list[0], ax=axes, orientation='vertical', fraction=0.05, pad=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb6a66",
   "metadata": {},
   "source": [
    "### Optional Step - Step 6: \n",
    "Create an Oceans3.0 account, and download a few hours of Sv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c99e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Request data from Oceans 3.0\n",
    "\n",
    "from onc.onc import ONC # This requires the ONC API Client Library package\n",
    "\n",
    "# You will require an ONC account and an ONC token to proceed\n",
    "# To register, visit: https://data.oceannetworks.ca/Registration\n",
    "#\n",
    "# Once registered:\n",
    "# 1) access your Profile from the top-right of https://data.oceannetworks.ca/\n",
    "# 2) Click the \"Web Services API\" tab\n",
    "# 3) Copy your token\n",
    "ONC_token = '12345678-abcdefgh-9012345-ijklmno' # Add your token here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: These files are very large, so just 2 hours of data can take a while to download\n",
    "\n",
    "#Initialize the ONC object for retrieving data\n",
    "onc = ONC(ONC_token)\n",
    "\n",
    "locationCode = 'FGPD'\n",
    "deviceCategoryCode = 'ECHOSOUNDERBIOA'\n",
    "deviceCode = 'BIOSONICSDTXU08003'\n",
    "start_time = '2019-06-28T00:00:00.000Z'\n",
    "end_time = '2019-06-28T02:00:00.000Z'\n",
    "ensemblePeriod = 0\n",
    "\n",
    "#Download in MAT :\n",
    "dataProductCode = 'BSTS' # 'BioSonics Time Series'\n",
    "filters = {\n",
    "    'locationCode':locationCode,\n",
    "    'deviceCategoryCode':deviceCategoryCode,\n",
    "    'dataProductCode':dataProductCode,\n",
    "    'extension': 'mat',\n",
    "    #'deviceCode':deviceCode,\n",
    "    'dateFrom': start_time,\n",
    "    'dateTo'  : end_time,\n",
    "    'dpo_ensemblePeriod': ensemblePeriod, # 0 = data not altered (none). All others in s: 20 (30s), 60, 300, 600, 900, 3600\n",
    "    'dpo_calibration': 1, # 1 = Sv. All others: 2=Ts, 0=rawCounts\n",
    "    'dpo_rangeAveraging':0, # 0=unaveraged, All others: 0.05, 0.1, 0.25, 0.5, 1\n",
    "    'token': ONC_token\n",
    "    }\n",
    "\n",
    "result = onc.requestDataProduct(filters)\n",
    "runData = onc.runDataProduct(result['dpRequestId'], waitComplete=True)\n",
    "print(runData)\n",
    "result = onc.downloadDataProduct(runData['runIds'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will have saved to a subfolder called \"output\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#Print the names of the downloaded files\n",
    "Sv_folder = Path(\"output\")  \n",
    "\n",
    "for f in Sv_folder.glob(\"*.mat\"):\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef221d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and plot the data\n",
    "# Plot the Sv left, and MVBS right\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Get the list of all mat-files in the data-directory\n",
    "mat_path = list(Sv_folder.glob('*.mat'))\n",
    "\n",
    "\n",
    "#Initialize some variables\n",
    "frequency = np.zeros(3)\n",
    "channels_Sv = []\n",
    "depth_bins = []\n",
    "\n",
    "#Go through all MAT files and load\n",
    "for mat_file in mat_path:\n",
    "    mat_dict = loadmat(mat_file,simplify_cells=True)\n",
    "    #print(mat_dict.keys())\n",
    "    data = mat_dict['data']\n",
    "    meta = mat_dict['meta']\n",
    "\n",
    "    #Get list info, and create keys\n",
    "    if frequency[0] == 0:\n",
    "        for ch in range(len(data)):\n",
    "            frequency[ch] = data[ch]['frequency']/1000\n",
    "            channels_Sv.append(\"ch_{}kHz\".format(int(frequency[ch])))\n",
    "            depth_bins.append(np.array(meta['depth'] - data[ch]['range']))\n",
    "\n",
    "        Sv = {k: [] for k in channels_Sv}\n",
    "        Sv_list = {k: [] for k in channels_Sv}\n",
    "        time_MATLAB = {k: [] for k in channels_Sv}\n",
    "        time_MATLAB_list = {k: [] for k in channels_Sv}\n",
    "        time_Sv = {k: [] for k in channels_Sv}\n",
    "\n",
    "    #Step through each channel in the data    \n",
    "    for ii,ch in enumerate(channels_Sv):\n",
    "        time_MATLAB_list[ch].append(np.array(data[ii]['time']))\n",
    "        Sv_list[ch].append(np.array(data[ii]['vals']))\n",
    "        #print(np.array(data[ii]['vals']).shape)\n",
    "\n",
    "\n",
    "#After loading from all files, concatenate the arrays\n",
    "for ch in channels_Sv:\n",
    "    Sv[ch] = np.concatenate(Sv_list[ch], axis=1)\n",
    "\n",
    "    #Also, concatenate the time values, and convert to datetime\n",
    "    # The offset for the Unix epoch (1970-01-01) in MATLAB datenum format is 719529\n",
    "    unix_epoch_offset_matlab = 719529\n",
    "    time_MATLAB[ch] = np.concatenate(time_MATLAB_list[ch])\n",
    "    time_Sv[ch] = pd.to_datetime(time_MATLAB[ch] - unix_epoch_offset_matlab, unit='D')\n",
    "\n",
    "#Delete the list variables to free up memory\n",
    "del Sv_list, data, time_MATLAB, time_MATLAB_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the MVBS and Sv side by side\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 1) Plot MVBS for the 3 channels\n",
    "# 2) Plot Sv for the 3 channels\n",
    "\n",
    "# Zoom to a date range\n",
    "start_zoom = datetime(2019, 6, 28)\n",
    "end_zoom = datetime(2019, 6, 28, 2, 0, 0)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Set background color for all subplots\n",
    "for ax in axes.flatten():\n",
    "    ax.set_facecolor([0.0, 0.0, 0.2])  # Dark Blue\n",
    "\n",
    "#Plot each channel of the MVBS data\n",
    "im_list = []\n",
    "for jj in range(2):\n",
    "    for ii, ch in enumerate(channels[0:3]):\n",
    "        if jj==0:\n",
    "            #Plot the MVBS data\n",
    "            im = axes[ii,0].imshow(MVBS[ch],\n",
    "                    extent = [time.min(), time.max(), depth_data.max(), depth_data.min()],\n",
    "                    aspect = 'auto',\n",
    "                    origin = 'lower',\n",
    "                    vmin=-70,\n",
    "                    vmax=-45,\n",
    "                    cmap='turbo')\n",
    "            \n",
    "        elif jj==1:\n",
    "             #Plot the Sv data\n",
    "             im = axes[ii,1].imshow(Sv[ch],\n",
    "                   extent = [time_Sv[ch].min(), time_Sv[ch].max(), depth_bins[ii].max(), depth_bins[ii].min()],\n",
    "                   aspect = 'auto',\n",
    "                   origin = 'lower',\n",
    "                   vmin=-70,\n",
    "                   vmax=-45,\n",
    "                   cmap='turbo')\n",
    "\n",
    "        im_list.append(im)  # keep reference for colorbar\n",
    "        \n",
    "        axes[ii,jj].set_title(ch)\n",
    "        axes[ii,jj].set_xlabel('Time')\n",
    "        axes[ii,jj].set_ylabel('Depth [m]')\n",
    "        axes[ii,jj].set_ylim(90, 0)\n",
    "        #Zoom to the days when the bloom in productivity appears to happen from the ABC data\n",
    "        axes[ii,jj].set_xlim(start_zoom, end_zoom)\n",
    "    \n",
    "\n",
    "# Add a single colorbar for all subplots\n",
    "fig.colorbar(im_list[0], ax=axes, orientation='vertical', fraction=0.05, pad=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a89b3",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- The original Sv data is very rich in detail that is lost with the averaged MVBS data\n",
    "- However, the MVBS data can be use to tell us where to look in the Sv data for interesting features and events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
